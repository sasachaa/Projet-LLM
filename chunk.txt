11 1.1, 8 The promise of Al Although we might have unrealistic short-term expectations for AI, the long-term picture is looking bright. We are only just getting started in applying deep learning to many important problems in which it could prove transtormative, from medical diagnoses to digital assistants, While AI research has been moving forward amazingly fast in the past five years, in large part due to a wave of funding never seen before in the short history of A.I, SO far relatively little of this progress has made its way into the products and processes that make up our world, Most of the research findings of deep learning are not yet applied, or at least not applied to the full range of problems that they can solve across all industries. Your doctor doesn't yet use AI, your accountant doesn't yet use AI. Yourself, you probably don't use AI technologies in your day-to-day life. Of course, you can ask simple questions to your smartphone and get reasonable answers, You can get fairly useful product recommendations on Amazon.com. You can search for "birthday" on Google Photos and instantly find those pictures of your daughter's birthday party from last month. That's a far cry from where such technologies used to stand. But such tools are still just accessory to our daily lives, AI has yet to transition to become central to the Right now it may seem hard to believe that AI could have a large impact on our world, because at this point AI is not yet widely deployed-much like it would have been difficult to believe in the future impact of the Internet back in 1995, Back then most people did not see how the Internet was relevant to them, how it was going to change their lives. The same is true for deep learning and AI today. But make no mistake: AI is coming. In a not SO distant future, AI will be your assistant, even your friend; it will answer your questions, it will help educate your kids, and it will watch over your health, Itwill deliver your groceries to your door and it will drive you from point A to point B. It will be your interface to an increasingly complex and increasingly information-intensive world, And even more importantly, AI will help humanity as a whole move forwards, by assisting human scientists in new breakthrough discoveries across all scientific fields, On the road to get there, we might face a few setbacks, and maybe a new AI winter in much the same way that the Internet industry got overhyped in 1998-1999 and suffered from a crash that dried up investment throughout the early 2000s, But we will get there eventually. AI will end up being applied to nearly every process that makes up Don't believe the short-term hype, but do believe in the long-term vision. It may take a while for AI to get deployed to its true potential a potential the full extent of which no one has yet dared to dream but AI is coming, and it will transform our world in a way we work, think and live, from genomics to mathematics. our society and our daily lives, much like the Internet today. fantastic way. OManning Publications Co. We welcome reader comments about anything in the manuscript other than typos and other simple mistakes. These will be cleaned up during production oft the book by copyeditors and proofreaders. htps/forums.manping comfiyums/dcp, i-aming-witb-python Licensed to null

12 1.2 Before deep learning: a brief history of machine learning Deep learning has reached a level of public attention and industry investment never seen before in the history of AI, but it isn't the first successful form of machine learning, In fact, it's a safe bet to say that most of the machine learning algorithms in use in the industry today are still not deep learning algorithms. Deep learning isn't always the right tool for the job-sometimes there just isn't enough data for deep learning to be applicable, and sometimes the problem is simply better solved by a different algorithm, If deep learning is your first contact with machine learning, then you may find yourselfin: a situation where all you have is the deep learning hammer and every machine learning problem starts looking like a nail for this hammer. The only way not to fall into this trap isto be familiar with other approaches and practice them when appropriate, A detailed exposure of classical machine learning approaches is outside of the scope ofthis book, but we will briefly go over them and describe the historical context in which they were developed. This will allow us to place deep learning in the broader context of machine learning, and better understand where deep learning comes from and why it matters. 1.2, 1 Probabilistic modeling Probabilistic modeling is the application of the principles of statistics to data analysis. It was one of the earliest forms of machine learning, yet it is still widely used to this day. One of the best-known algorithms in this category is the Naive Bayes algorithm, Naive Bayes is a type of machine learning classifier based on applying the Bayes Theorem while assuming that the features in the input data are all independent (a strong, or "naive" assumption, which is where the name comes from). This form of data analysis actually predates computers, and was applied by hand decades before its first computer implementation (most likely dating back to the 1950s), The Bayes Theorem and the foundations of statistics themselves date back to the 18th century, and these are all you A closely related model is the Logistic Regression (logreg for short), which is sometimes considered to be the "hello world" of modern machine learning. Don't be misled by its name logreg is in fact a classification algorithm rather than a regression algorithm, Much like Naive Bayes, logreg predates computing by a long time, yet it is still very useful to this day, thanks to its simple and versatile nature. It is often the first thing a data scientist will try on a dataset to get a feel for the classification task at hand. need to start using Naive Bayes classifiers. OManning Publications Co, We welcome reader comments about anything in the manuscript other than typos and other simple mistakes. These will be cleaned up during production oft the book by copyeditors and proofreaders. htps/forums.manping comfiyums/dcp.i-aming-witb-python Licensed to null

13 1.2, 2 Early neural networks Early iterations of neural networks have been completely supplanted by the modern variants that we cover in these pages; however, it is helpful to be aware of how deep learning originated. Although the core ideas of neural networks were investigated in toy forms as early as the 1950s, the approach took decades to really get started, For a long time, the missing piece was a lack ofan efficient way to train large neural networks, This changed in the mid-1980s, as multiple people independently rediscovered the "backpropagation" algorithm, a way to train chains of parametric operations using gradient descent optimization (later in the book, we will go on to precisely define these The first successful practical application of neural nets came in 1989 from Bell Labs, when Yann LeCun combined together the earlier ideas of convolutional neural networks and backpropagation, and applied them to the problem of handwritten digits classification, The resulting network, dubbed "LeNet", was used by the US Post Office in concepts), and started applying it to neural networks. the 1990s to automate the reading of ZIP codes on mail envelopes. 1.2, 3 Kernel methods As neural networks started gaining some respect among researchers in the 1990s thanks to this first success, a new approach to machine learning rose to fame and quickly sent Kernel methods are a group of classification algorithms, the best known of which is the Support Vector Machine (SVM), The modern formulation of SVM was developed by Vapnik and Cortes in the early 1990s at Bell Labs and published in 1995, although an older linear formulation was published by Vapnik and Chervonenkis as early as 1963. SVM aims at solving classification problems by finding good "decision boundaries" (Figure 1.10) between two sets of points belonging to two different categories, A "decision boundary" can be thought of as a line or surface separating your training data into two spaces corresponding to two categories. To classify new data points, you just neural nets back to oblivion: kernel methods. need to check which side of the decision boundary they fall on. Figure 1.10 A decision boundary OManning Publications Co, We welcome reader comments about anything in the manuscript other than typos and other simple mistakes. These will be cleaned up during production oft the book by copyeditors and proofreaders. htps/forums.manping comfiyums/dcp, i-aming-witb-python Licensed to null

14 SVMs proceed to find these boundaries in two steps: First, the data is mapped to a new high-dimensional representation where the decision boundary can be expressed as an hyperplane (ift the data is two-dimensional like in our Then a good decision boundary (a separation hyperplane) is computed by trying to maximize the distance between the hyperplane and the closest data points from each class, a step called'maximizing the margin", This allows the boundary to generalize well The technique of mapping data to a high-dimensional representation where a classification problem becomes simpler may look good on paper, but in practice it is often computationally intractable. That's where the "kernel trick" comes in, the key idea that kernel methods are named after, Here's the gist of it: for finding good decision hyperplanes in the new representation space, you don't have to explicitly compute the coordinates of your points in the new space, you just need to compute the distance between pairs of points in that space, which can be done very efficiently using what is called a "kernel function", A kernel function is a computationally tractable operation that maps any two points in your initial space to the distance between these points in your target representation space, completely by-passing the explicit computation of the new representation, Kernel functions are typically crafted by hand rather than learned from At the time they were developed, SVMs exhibited state of the art performance on simple classification problems, and were one of the few machine learning methods backed by extensive theory and amenable to serious mathematical analysis, making it well-understood and easily interpretable, Because of these useful properties, it became However, SVM proved hard to scale to large datasets and did not provide very good results for "perceptual" problems such as image classification, Since SVM is a "shallow" method, applying SVM to perceptual problems requires first extracting useful representations manually (a step called "feature engineering"), which is difficult and example, an "hyperplane" would simply be a straight line). tor new samples outside of the training dataset. data -in the case of SVM, only the separation hyperplane is learned, extremely popular in the field for a long time. brittle. 1.2.4 Decision trees, Random Forests and Gradient Boosting Machines Decision trees are flowchart-like structures that can allow to classify input data points or predict output values given inputs. They are easy to visualize and interpret, Decisions trees learned from data started getting significant research interest in the 2000s, and by 2010 they were often preferred to kernel methods. OManning Publications Co. We welcome reader comments about anything in the manuscript other than typos and other simple mistakes. These will be cleaned up during production oft the book by copyeditors and proofreaders. htps/forums, manping comfiyums/dcp.i-aming-witb-python Licensed to null

15 Input data Question Question Question Category Category Category Category Figure 1.11 A decision tree: the parameters that are learned are the questions about the data. A question could be, for instance, "is coefficient 2 in the data higher than 3.5?", In particular, the "Random Forest" algorithm introduced a robust and practical take on decision tree learning that involves building a large number of specialized decision trees then ensembling their outputs. Random Forests are applicable to a very wide range of problems --you could say that they are almost always the second-best algorithm for any shallow machine learning task, When the popular machine learning competition website Kaggle.com got started in 2010, Random Forests quickly became a favorite on the platform-until 2014, when Gradient Boosting Machines took over. Gradient Boosting Machines, much like Random Forests, is a machine learning technique based on ensembling weak prediction models, generally decision trees, It leverages "gradient boosting", a way to improve any machine learning model by iteratively training new models that specialize in addressing the weak points of the previous models. Applied to decision trees, the use of the "gradient boosting" technique results in models that strictly outperform Random Forests most of the time, while having very similar properties, It may be one of the best, if not the best, algorithm for dealing with non-perceptual data today. Alongside deep learning, it is one of the most commonly used technique in Kaggle competitions. 1.2, 5 Back to neural networks Around 2010, while neural networks were almost completely shunned by the scientific community at large, a number of people still working on neural networks started making important breakthroughs: the groups of Geoffrey Hinton at the University of Toronto, Yoshua Bengio at the University of Montreal, Yann LeCun at New York University, and In 2011, Dan Ciresan from IDSIA started winning academic image classification competitions with GPU-trained deep neural networks the first practical success of modern deep learning, But the watershed moment came in 2012, with the entry of Hinton's group in the yearly large-scale image classification challenge ImageNet. The ImageNet challenge was notoriously difficult at the time, consisting in classifying high-resolution color images into 1000 different categories after training on 1.4 million images, In 2011, the top-5 accuracy of the winning model, based on classical approaches OManning Publications Co. We welcome reader comments about anything in the manuscript other than typos and other simple mistakes. These will be cleaned up during production oft the book by copyeditors and proofreaders. IDSIA in Switzerland. htps/Hiorms.mamppg. comfiyums/dcp, i-aming-witb-python Licensed to null

16 to computer vision, was only 74.3%. Then in 2012, a team led by Alex Krizhevsky and advised by Geoffrey Hinton was able to achieve a top-5 accuracy of83.6%-a significant breakthrough. The competition has been dominated by deep convolutional neural networks every year since. By 2015, we had reached an accuracy of 96, 4%, and the classification task on ImageNet was considered to be a completely solved problem. Since 2012, deep convolutional neural networks ("convnets") have become the go-to algorithm for all computer vision tasks, and generally all perceptual tasks, At major computer vision conferences in 2015 or 2016, it had become nearly impossible to find presentations that did not involve convnets in some form. At the same time, deep learning has also found applications in many other types of problems, such as natural language processing. It has come to completely replace SVMs and decision trees in a wide range of applications, For instance, for several years, the European Organization for Nuclear Research, CERN, used decision tree-based methods for analysis of particle data from the ATLAS detector at the Large Hadron Collider (LHC), but they eventually switched to Keras-based deep neural networks due to their higher performance and ease of training on large datasets. 1.2, 6 What makes deep learning different The reason why deep learning took off SO quickly is primarily that it offered better performance on many problems. But that's not the only reason. Deep learning is also making problem-solving much easier, because it completely automates what used to be the most crucial step in a machine learning workflow: "feature engineering", Previous machine learning techniques, "shallow" learning, only involved transforming the input data into one or two successive representation spaces, usually via very simple transformations such as high-dimensional non-linear projections (SVM) or decision trees. But the refined representations required by complex problems generally cannot be attained by such techniques, As such, humans had to go to great length to make the initial input data more amenable to processing by these methods, i.e. they had to manually engineer good layers of representations for their data, This is what is called "feature engineering" Deep learning, on the other hand, completely automates this step: with deep learning, you learn all features in one pass rather than having to engineer them yourself. This has greatly simplified machine learning workflows, often replacing very sophisticated multi-stage pipelines with a single, simple, end-to-end deep learning model, You may ask, if the crux of the issue is to have multiple successive layers of representation, could shallow methods be applied repeatedly to emulate the effects of deep learning? In practice, there are fast-diminishing returns to successive application of shallow learning methods, because the optimal first representation layer in a 3-layer model is not the optimal first layer in a 1-layer or 2-layer model, What is transformative about deep learning is that it allows a model to learn all layers ofrepreseniation/oiuiy, at the same time, rather than in succession ("greedily", as it is called). With joint feature learning, whenever the model adjusts one of its internal features, all other features that depend on it will automatically adapt to the change, without requiring human OManning Publications Co, We welcome reader comments about anything in the manuscript other than typos and other simple mistakes. These will be cleaned up during production oft the book by copyeditors and proofreaders. htps/Horms.manping. comfiyums/dcp.i-aming-witb-python Licensed to null